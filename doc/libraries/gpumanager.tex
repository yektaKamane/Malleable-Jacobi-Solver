\setcounter{secnumdepth}{3}

\renewcommand{\code}[1]{\texttt{\textbf{#1}}}

\newcommand{\cuda}{\code{CUDA}}

\section{Overview}

GPU Manager is a task offload and management library for efficient use of
CUDA-enabled GPUs in \charmpp{} applications. CUDA code can be integrated
in \charmpp{} just like any \CC{} program, but the resulting performance
is likely to be far from ideal. This is because overdecomposition, a core
concept of \charmpp{}, creates fine-grained objects and tasks which causes
problems on the GPU.

GPUs are throughput-oriented devices with peak computational capabilities that
greatly surpass equivalent-generation CPUs but with limited control logic. This
currently constrains them to be used as accelerator devices controlled by code
on the CPU. Traditionally, programmers have had to either (a) halt the execution
of work on the CPU whenever issuing GPU work to simplify synchronization or (b)
issue GPU work asynchronously and carefully manage and synchronize concurrent
GPU work in order to ensure progress and good performance. The latter option,
which is practically a requirement in \charmpp{} to preserve asynchrony, becomes
significantly more difficult with numerous concurrent objects that issue kernels
and data transfers to the GPU.

The \charmpp{} programmer is strongly recommended to use CUDA streams to
mitigate this problem, by assigning separate streams to chares. This allows
operations in different streams to execute concurrently. It should be noted that
concurrent data transfers are limited by the number of DMA engines, and current
GPUs have one per direction of the transfer (host-to-device, device-to-host).
The concurrent kernels feature of CUDA allows multiple kernels to execute
simultaneously on the device, as long as resources are available.

An important factor of performance with using GPUs in \charmpp{} is that the CUDA
API calls invoked by chares to offload work should be non-blocking. The chare
that just offloaded work to the GPU should yield the PE so that other chares
waiting to be executed can do so. Unfortunately, many CUDA API calls used to
wait for completion of GPU work, such as \code{cudaStreamSynchronize} and
\code{cudaDeviceSynchronize}, are blocking. Since the PEs in \charmpp{} are
implemented as persistent kernel-level threads mapped to each CPU core, this
means other chares cannot run until the GPU work completes and the blocked
chare finishes executing. To resolve this issue, GPU Manager provides Hybrid API
(HAPI) to the \charmpp{} user, which includes new functions to implement the
non-blocking features and a set of wrappers to the CUDA runtime API functions.
The non-blocking API allows the user to specify a \charmpp{} callback upon offload
which will be invoked when the operations in the CUDA stream are complete.

\section{Building GPU Manager}

GPU Manager is not included by default when building \charmpp{}. In order to use
GPU Manager, the user must build \charmpp{} using the \cuda{} option, e.g.

\begin{alltt}
./build charm++ netlrts-linux-x86_64 cuda -j8
\end{alltt}

Building GPU Manager requires an installation of the CUDA toolkit on the system.

\section{Using GPU Manager}

As explained in the Overview section, use of CUDA streams is strongly recommended.
This allows kernels offloaded by chares to execute simultaneously on the GPU,
which boosts performance if the kernels are small enough for the GPU to be able
to allocate resources.

In a typical \charmpp{} application using CUDA, \code{.C} and \code{.ci} files
would contain the \charmpp{} code, whereas a \code{.cu} file would include the
definition of CUDA kernels and a function that serves as an entry point from
the \charmpp{} application to use GPU capabilities. CUDA/HAPI calls for data
transfers or kernel invocations would be placed inside this function, although
they could also be put in a \code{.C} file provided that the right header files
are included (\code{<cuda_runtime.h> or "hapi.h"}). The user should make sure
that the CUDA kernel definitions are compiled by \code{nvcc}, however.

After the necessary data transfers and kernel invocations, \code{hapiAddCallback}
would be placed where typically \code{cudaStreamSynchronize} or
\code{cudaDeviceSynchronize} would go. This informs the runtime that a chare has
offloaded work to the GPU, allowing the provided \charmpp{} callback to be
invoked once it is complete. The non-blocking API has the following prototype:

\begin{alltt}
  void hapiAddCallback(cudaStream_t stream, CkCallback* callback);
\end{alltt}

Other HAPI calls:

\begin{alltt}
  void hapiCreateStreams();
  cudaStream_t hapiGetStream();

  cudaError_t hapiMalloc(void** devPtr, size_t size);
  cudaError_t hapiFree(void* devPtr);
  cudaError_t hapiMallocHost(void** ptr, size_t size);
  cudaError_t hapiFreeHost(void* ptr);

  void* hapiPoolMalloc(int size);
  void hapiPoolFree(void* ptr);

  cudaError_t hapiMemcpyAsync(void* dst, const void* src, size_t count,
                              cudaMemcpyKind kind, cudaStream_t stream = 0);

  hapiCheck(code);
\end{alltt}

\code{hapiCreateStreams} creates as many streams as the maximum number of
concurrent kernels supported by the GPU device. \code{hapiGetStream} hands
out a stream created by the runtime in a round-robin fashion. The
\code{hapiMalloc} and \code{hapiFree} functions are wrappers to the corresponding
CUDA API calls, and \code{hapiPool} functions provides memory pool functionalities
which are used to obtain/free device memory without interrupting the GPU.
\code{hapiCheck} is used to check if the input code block executes without errors.
The given code should return \code{cudaError_t} for it to work.

Example \charmpp{} applications using CUDA can be found under
\code{examples/charm++/cuda}. Codes under #ifdef USE_WR use the workRequest
scheme, which is now deprecated.
